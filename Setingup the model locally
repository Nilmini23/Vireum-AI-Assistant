pip install transformers accelerate torch sentencepiece
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

def chat(prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(**input_ids, max_new_tokens=300, temperature=0.7)
    return tokenizer.decode(output[0], skip_special_tokens=True)

print(chat("You are an AI assistant for Vireum Oy. Estimate cost and time for an AI chatbot project."))
